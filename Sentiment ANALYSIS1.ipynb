{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing library\n",
    "import nltk\n",
    "import re \n",
    "import string\n",
    "from os import listdir\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter # is to get the counts of all the words from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() to download the NLTK corpous if not avilable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading a negtive review text file\n",
    "text_path = open(\"data/neg/cv000_29416.txt\")\n",
    "text = text_path.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the file with function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_name):\n",
    "    file = open(file_name,\"r\")\n",
    "    text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"data/neg/cv000_29416.txt\"\n",
    "text = load_text(path)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "#     print(\"length of text file before cleaning {}\".format(len(text)))\n",
    "    tokens = word_tokenize(text)                                                # using word tokenizing \n",
    "    re_punct = re.compile(\"[%s]\"%re.escape(string.punctuation))                 # for rmoving puncations\n",
    "    tokens = [re_punct.sub(\"\",w) for w in tokens]\n",
    "    tokens = [w for w in tokens if w.isalpha()]                                 # takinf only alpha numeric\n",
    "    stop_words = set(stopwords.words(\"english\"))                                \n",
    "    tokens = [w for w in tokens if w not in stop_words]                         # removing stopwords\n",
    "    tokens = [w.lower() for w in tokens]                                    # normalizing(making all words to lower case)\n",
    "    tokens = [w for w in tokens if len(w)>1]                                  # removing words length greater than 1\n",
    "#     print(\"length of text file after cleaning {}\".format(len(tokens)))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(text) #c:\\cmr_python36\\lib\\nltk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a bag of words(dictnory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # is to get the counts of all the words from \n",
    "vocab = Counter()                     # vocabalorary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(filename):\n",
    "    text=load_text(filename)                        # calling the load_text function\n",
    "    tokens = clean_text(text)                       # calling clean_text function\n",
    "    vocab.update(tokens)                            # updating the words to vocab\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the directory of all the files for creating dictnory\n",
    "def process_doc(directory):\n",
    "    for filename in listdir(directory):\n",
    "        path = directory+\"/\"+filename\n",
    "        add_doc_to_vocab(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_neg = \"data/neg\"\n",
    "path_pos = \"data/pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_doc(path_neg)\n",
    "process_doc(path_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [k for k,c in vocab.items() if c>=2] # vocab of words greater then length 2\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(vocab,filename):\n",
    "    data = \"\\n\".join(vocab)\n",
    "    file = open(filename,\"w\")\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(vocab,\"data/vocablatest.txt\") # saving the vocab words to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1 = load_text(\"data/vocablatest.txt\")\n",
    "print(len(vocab1))\n",
    "vocab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now taking the words in the data which are present in the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functin for getting the words from data which are present in bag of words \n",
    "def add_doc_to_vocab(filename):\n",
    "    text=load_text(filename)\n",
    "    tokens = clean_text(text)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the add_doc_to_vocab for all the files\n",
    "def preprocess_doc(directory,vocab): \n",
    "    lines = []\n",
    "    for filename in listdir(directory): \n",
    "        path = directory+\"/\"+filename\n",
    "        line = add_doc_to_vocab(path)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"data/vocablatest.txt\"\n",
    "vocab = load_text(vocab)\n",
    "vocab = set(vocab.split())\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the cleaned data from the all the files\n",
    "def load_clean_data(vocab):\n",
    "    neg=preprocess_doc(\"data/neg\",vocab)\n",
    "    pos=preprocess_doc(\"data/pos\",vocab)\n",
    "    docs = neg+pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]  # \"0\" means negative review \"1\" means positive review\n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs,labes = load_clean_data(vocab)          # calling the load_clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs),len(labes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labes[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dividing the data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the data into test and train based on the file name which starts with cv9(100 in positive 100 in negative)\n",
    "def preprocess_doc(directory,vocab,is_train):\n",
    "    lines = []\n",
    "    for filename in listdir(directory):\n",
    "        if is_train and filename.startswith(\"cv9\"):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith(\"cv9\"):\n",
    "            continue\n",
    "        path = directory+\"/\"+filename\n",
    "        line = add_doc_to_vocab(path)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_test_train_data(vocab,is_train):\n",
    "    neg=preprocess_doc(\"data/neg\",vocab,is_train)\n",
    "    pos=preprocess_doc(\"data/pos\",vocab,is_train)\n",
    "    docs = neg+pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))] # \"0\" means negative review \"1\" means positive review\n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs,train_labels = load_clean_test_train_data(vocab,True)\n",
    "test_docs,test_labels = load_clean_test_train_data(vocab,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_docs),len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_docs),len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "tokenizerss = create_tokenizer(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = tokenizerss.texts_to_matrix(train_docs,mode = \"freq\")\n",
    "Xtest = tokenizerss.texts_to_matrix(test_docs,mode = \"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving for the prediction \n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizerss, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentement analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,input_shape=(input_shape,),activation=\"relu\"))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")\n",
    "#     model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrices=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = Xtrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = s_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "print(test_labels.shape),print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(Xtrain,train_labels,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss=model.evaluate(Xtest,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"SAMODEL.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model1 = keras.models.load_model('SAMODEL.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "def load_tokinezer():\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review,vocab,tokenizer,model):\n",
    "    tokens = review.split()\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = \" \".join(tokens)\n",
    "    tokenizer = load_tokinezer()\n",
    "    \n",
    "    encoded = tokenizer.texts_to_matrix([line],mode = \"freq\")\n",
    "    yhat = model.predict(encoded)\n",
    "    precent_pos=yhat[0,0]\n",
    "    if round(precent_pos)==0:\n",
    "        return (1-precent_pos), \"NAGITIVE\"\n",
    "    return precent_pos,\"POSITIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(percent,sentement) = predict_sentiment(text,vocab,tokenizer,model1)\n",
    "print(percent,sentement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this movie is a great movie enjoyed a lot\"\n",
    "percent,sentement = predict_sentiment(text,vocab,tokenizer,model1)\n",
    "print( \"sentement of the review is -> \"+str(sentement) +\" and condifent score is = \"+str(percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"movie was a horrible and there was no thriller\"\n",
    "percent,sentement = predict_sentiment(text,vocab,tokenizer,model1)\n",
    "print(percent),print(sentement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run streamlit.py# for ui to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
